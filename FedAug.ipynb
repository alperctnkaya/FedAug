{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FedAug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFXutZgUsW0H"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc5xTsoXOF3Z"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils import data as torch_data\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch import flatten\n",
        "\n",
        "import math\n",
        "import logging\n",
        "\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc6ChkgzJTTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f3d5b2-4557-4055-a3db-8a709137c7db"
      },
      "source": [
        "drive.mount(\"my-drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at my-drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idm9FO5aMiOo"
      },
      "source": [
        "!cp /content/my-drive/MyDrive/Chest_Dataset.zip /media/\n",
        "!unzip /media/Chest_Dataset.zip -d /media/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e329fGpKJc0S"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evGxAxPj5Z5t"
      },
      "source": [
        "def get_training_data(args):\n",
        "    data = [] \n",
        "    for label in args.labels: \n",
        "        path = os.path.join(args.data_dir, label)\n",
        "        class_num = args.labels.index(label)\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (args.img_size, args.img_size))/255. # Reshaping images to preferred size\n",
        "                data.append([resized_arr.astype(np.float32), class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKVRUGmL5l-q"
      },
      "source": [
        "class Dataset_V2(torch_data.Dataset):\n",
        "  def __init__(self, data, transforms):\n",
        "    self.transform = transforms\n",
        "    self.x = data[:,0]\n",
        "    self.y = data[:,1]\n",
        "\n",
        "    self.labels, self.counts = np.unique(self.y, return_counts= True)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.transform(self.x[idx]) , self.y[idx]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy2Aem135sWI"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 48, kernel_size=7, stride=2, padding=1)\n",
        "    self.conv2 = nn.Conv2d(48, 64, kernel_size=5, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(48)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "    self.bn5 = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.fc1 = nn.Linear(3*3*256, 128)\n",
        "    self.fc2 = nn.Linear(128, 32)\n",
        "    self.fc3 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.bn1(x)\n",
        "\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.bn3(x)\n",
        "    x = self.dropout(x) #do1\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.bn4(x)\n",
        "    x = self.dropout(x) #do2\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = F.relu(self.conv5(x))\n",
        "    x = self.bn5(x)\n",
        "    x = self.dropout(x) #do3\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = flatten(x, start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_MsZ4A1PNMd"
      },
      "source": [
        "class Client():\n",
        "  def __init__(self, Id, args, data=None):\n",
        "    self.Id = Id\n",
        "    self.data = data\n",
        "    self.dataLoader = None\n",
        "    \n",
        "    self.batchSize = args.batch_size\n",
        "    self.lr = args.lr\n",
        "    self.device = args.device\n",
        "\n",
        "    self.model = Net().to(self.device)\n",
        "\n",
        "    if(data is not None):\n",
        "      self.dataset = self.assignDataset(dataset)\n",
        "\n",
        "    self.augmentedData = None\n",
        "    self.augmentedDataset = None \n",
        "    self.augmentedDataLoader = None \n",
        "    \n",
        "  def assignData(self, data, dataType):\n",
        "    if(dataType == \"Original\"):\n",
        "      self.data = data\n",
        "      self.dataset = Dataset_V2(self.data, transforms.ToTensor())\n",
        "      self.dataLoader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batchSize)\n",
        "    \n",
        "    elif(dataType == \"Augmented\"):\n",
        "      self.augmentedData = data\n",
        "      self.augmentedDataset = Dataset_V2(self.augmentedData, transforms.ToTensor())\n",
        "      self.augmentedDataLoader = torch.utils.data.DataLoader(self.augmentedDataset, batch_size=self.batchSize)\n",
        "  \n",
        "  def localTrain(self, dataType, globalModel, localEpochs, criterion):\n",
        "    device = self.device\n",
        "\n",
        "    self.model.load_state_dict(globalModel.state_dict())\n",
        "    self.model.train()\n",
        "\n",
        "    optimizer = torch.optim.Adadelta(self.model.parameters(), lr = self.lr)\n",
        "\n",
        "    sampleCount = len(self.dataset) if dataType == \"Original\" else len(self.augmentedDataset) \n",
        "    loader = self.dataLoader if dataType == \"Original\" else self.augmentedDataLoader\n",
        "    update = {}\n",
        "    correct = 0\n",
        "    \n",
        "    for epoch in range(localEpochs):\n",
        "      epochCorrect = 0\n",
        "      epochLoss = 0.0\n",
        "      \n",
        "      for batchIdx, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(self.device), y.to(self.device)\n",
        "        output = self.model(x)\n",
        "        batchLoss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        batchLoss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        epochCorrect += pred.eq(y.view_as(pred)).sum().item()\n",
        "        epochLoss += batchLoss.item()\n",
        "\n",
        "      correct += epochCorrect\n",
        "      \n",
        "      print('LOCAL_EPOCH: {}  Client: {} \\tLoss: {:.6f} Accuracy: {}/{} ({:.3f}%)'.format(\n",
        "              epoch, self.Id, epochLoss / (len(loader)), epochCorrect, sampleCount, (100. * epochCorrect) / sampleCount))\n",
        "      \n",
        "    update[\"update\"] = self.model.state_dict()\n",
        "    update[\"sampleCount\"] = sampleCount * localEpochs\n",
        "    return update"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTOV8ZrJMFr9"
      },
      "source": [
        "class FLSimulator():\n",
        "  def __init__(self, train_set, args, distribution = \"IID\"):\n",
        "    self.num_clients = args.num_clients\n",
        "    self.clients = self.create_client_devices(self.num_clients, args)\n",
        "    self.train_set = train_set\n",
        "    self.s = splitter(self.train_set)\n",
        "    self.args = args\n",
        "\n",
        "    if distribution == \"IID\":\n",
        "      self.s.IID(self.clients)\n",
        "    elif distribution ==\"Non_IID\":\n",
        "      self.s.Non_IID(self.clients)\n",
        "    elif distribution == \"quantity_skew\":\n",
        "      self.s.quantitySkew(self.clients)\n",
        "\n",
        "\n",
        "  def create_client_devices(self, num_clients, args):\n",
        "    clients = {}\n",
        "    for i in range(num_clients):\n",
        "      clients[\"VW_\" + str(i)] = Client(\"VW_\" + str(i), args)\n",
        "      \n",
        "    return clients \n",
        "  \n",
        "  def print_client_distributions(self):\n",
        "    for client in self.clients:\n",
        "      print(self.clients[client].dataset.counts)\n",
        "    \n",
        "  def plotDataDistribution(self):  \n",
        "    maxLabelCount = 0\n",
        "    for client in self.clients:\n",
        "      if len(self.clients[client].dataset.labels) >= maxLabelCount:\n",
        "        maxLabelCount = len(self.clients[client].dataset.labels)\n",
        "\n",
        "    values = np.ones((len(self.clients), maxLabelCount))\n",
        "    for idx, client in enumerate(self.clients):\n",
        "      values[idx] = self.clients[client].dataset.counts\n",
        "    \n",
        "    x = [\"C_\" + str(i) for i in range(1,len(self.clients)+1)]\n",
        "    \n",
        "    plt.figure(figsize=(12, 6), dpi=100)\n",
        "\n",
        "    plt.bar(x, values[:,0], width=0.3)\n",
        "    plt.bar(x, values[:,1], 0.3, bottom = values[:,0])\n",
        "    plt.bar(x, values[:,2], 0.3, bottom = values[:,0] + values[:,1])\n",
        "    plt.bar(x, values[:,3], 0.3, bottom = values[:,0] + values[:,1] + values[:,2])\n",
        "    \n",
        "    plt.legend(self.args.labels)\n",
        "    \n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"# of Samples\")\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LDJ0-xAfNhg"
      },
      "source": [
        "class server():\n",
        "  def __init__(self, test_set, clients, args):\n",
        "    self.globalModel = Net().to(device)\n",
        "    self.clients = clients\n",
        "    self.num_clients_round = args.num_clients_round\n",
        "    self.test_set = test_set\n",
        "    self.test_loader = torch_data.DataLoader(Dataset_V2(self.test_set, transforms.ToTensor()), shuffle=False)\n",
        "    self.b = balancer()\n",
        "    self.aggregator = FederatedAggregation()\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.args = args\n",
        "\n",
        "  def federated_train(self, num_comm_round):\n",
        "    \n",
        "    test_accuracies = []\n",
        "    test_lossess = []\n",
        "    \n",
        "    for round in range(num_comm_round):\n",
        "      print(\"communication round: \" + str(round))\n",
        "      selected_clients = self.random_select_clients(self.clients, self.args.num_clients_round)\n",
        "      self.b.balanceClientsDistribution(self.clients, selected_clients)\n",
        "      \n",
        "      roundUpdates = []\n",
        "      for client in selected_clients:\n",
        "        roundUpdates.append(self.clients[client].localTrain(\"Augmented\", self.globalModel, args.num_local_epochs, self.criterion))  \n",
        "      self.aggregator.aggregate(self.globalModel, roundUpdates)\n",
        "      test_loss, test_accuracy = self.test()\n",
        "      test_accuracies.append(test_accuracy)\n",
        "      test_lossess.append(test_lossess)\n",
        "    \n",
        "    return test_accuracies,  test_lossess\n",
        "\n",
        "  def random_select_clients(self, clients, number):\n",
        "    return random.sample(list(clients), number)\n",
        "\n",
        "  def test(self):\n",
        "    self.globalModel.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in self.test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = self.globalModel(x)\n",
        "            test_loss += self.criterion(output, y).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(self.test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / (len(self.test_loader)* self.test_loader.batch_size) \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(self.test_loader) * self.test_loader.batch_size, test_accuracy))\n",
        "    \n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R66qhFr7mMjh"
      },
      "source": [
        "class FederatedAggregation():\n",
        "  def __init__(self, args=None):\n",
        "    self.args = args\n",
        "  \n",
        "  def aggregate(self, globalModel, clientUpdates):\n",
        "    aggregatedUpdates = self.FedAvg(clientUpdates)\n",
        "    globalModel.load_state_dict(aggregatedUpdates)\n",
        "\n",
        "  def FedAvg(self, updates):\n",
        "    layers = updates[0][\"update\"].keys()\n",
        "    aggregatedUpdates = dict.fromkeys(layers)\n",
        "    \n",
        "    totalSample = 0\n",
        "    for update in updates:\n",
        "      for layer in layers:\n",
        "        if(aggregatedUpdates[layer] is None):\n",
        "          aggregatedUpdates[layer] = update[\"sampleCount\"] * update[\"update\"][layer]\n",
        "        else:\n",
        "          aggregatedUpdates[layer] += update[\"sampleCount\"] * update[\"update\"][layer]\n",
        "      totalSample += update[\"sampleCount\"]\n",
        "    \n",
        "    for layer in layers:\n",
        "      try:\n",
        "        aggregatedUpdates[layer] /= totalSample\n",
        "      except:\n",
        "        aggregatedUpdates[layer] = torch.tensor(aggregatedUpdates[layer] / totalSample)\n",
        "    \n",
        "    return aggregatedUpdates"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX77UJ1aac2G"
      },
      "source": [
        "class splitter():\n",
        "  def __init__(self, dataset, dirichletParams = 1):\n",
        "    self.dataset = dataset\n",
        "    self.dirichletParams = dirichletParams\n",
        "\n",
        "  def IID(self, clients):\n",
        "    train_set = self.dataset\n",
        "    len_train_set = len(train_set)\n",
        "    len_client_set = int(len_train_set / len(clients))\n",
        "\n",
        "    for i, client in enumerate(clients): \n",
        "      dataset = train_set[i*len_client_set:(i+1)*len_client_set]\n",
        "      clients[client].assignData(dataset, \"Original\")\n",
        "\n",
        "    return clients\n",
        "\n",
        "  def quantitySkew(self, clients):\n",
        "    train_set = self.dataset\n",
        "    len_train_set = len(train_set)\n",
        "    num_clients = len(clients)\n",
        "    \n",
        "    intervals = np.floor(np.random.dirichlet(num_clients*(self.dirichletParams,), 1)*len_train_set).astype(int).squeeze()\n",
        "    count = 0\n",
        "    for i, client in enumerate(clients): \n",
        "      dataset = train_set[count : count + intervals[i]]\n",
        "      clients[client].assignData(dataset, \"Original\")\n",
        "      count += intervals[i]\n",
        "\n",
        "    return clients\n",
        "\n",
        "  def Non_IID(self, clients):\n",
        "    train_set = self.dataset\n",
        "    len_train_set = len(train_set)\n",
        "    num_clients = len(clients)\n",
        "    \n",
        "    intervals = {}\n",
        "    labels, counts = np.unique(train_set[:,1], return_counts= True)\n",
        "    \n",
        "    sample_counts = {}\n",
        "    \n",
        "    for label, count in zip(labels, counts):\n",
        "      intervals[label] = (np.random.dirichlet(num_clients*(self.dirichletParams,),1)*count).astype(int).squeeze()\n",
        "      intervals[label] = np.where(intervals[label] == 0, 1, intervals[label])\n",
        "      sample_counts[label] = 0\n",
        "    \n",
        "    for i, client in enumerate(clients):\n",
        "      dataset = np.empty_like(train_set[0:0])\n",
        "      for label in labels:      \n",
        "        dataset = np.concatenate((dataset, train_set[train_set[:,1] == label][sample_counts[label]:sample_counts[label]+intervals[label][i]]), axis=0)\n",
        "        sample_counts[label] += intervals[label][i]\n",
        "\n",
        "      np.random.shuffle(dataset)\n",
        "      clients[client].assignData(dataset, \"Original\")\n",
        "\n",
        "    return clients"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xRFkd7Vrbu"
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "  def __init__(self, mean=0., std=0.015):\n",
        "    self.std = std \n",
        "    self.mean = mean \n",
        "        \n",
        "  def __call__(self, tensor):\n",
        "    return (tensor + torch.randn(tensor.size())*self.std + self.mean).squeeze()\n",
        "    \n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSau-mQaMKdu"
      },
      "source": [
        "class augTransforms():\n",
        "  def __init__(self):\n",
        "    self.HFlip = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomHorizontalFlip(p=1),transforms.ToTensor()])\n",
        "    self.VFlip = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomVerticalFlip(p=1),transforms.ToTensor()])\n",
        "    self.RRotation = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomRotation(degrees = (-30,30)),transforms.ToTensor()])\n",
        "    self.RCrop = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomCrop((128,128), padding=28),transforms.ToTensor()])\n",
        "    self.CJitter = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.ColorJitter(brightness=.3, contrast=.3, saturation=.3,hue=.3),transforms.ToTensor()])\n",
        "    self.RInvert = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomInvert(p=1),transforms.ToTensor()])\n",
        "    self.RAdjustSharpness = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomAdjustSharpness(sharpness_factor=3, p=1),transforms.ToTensor()])\n",
        "    self.RAutoContrast = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomAutocontrast(p=1), transforms.ToTensor()])\n",
        "    self.REqualize = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomEqualize(p=1), transforms.ToTensor()])\n",
        "    self.RAffine = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomAffine(degrees=(-20,20)),transforms.ToTensor()])\n",
        "    self.RPerspective = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomPerspective(distortion_scale=0.2,p=1), transforms.ToTensor()])\n",
        "    self.GBlur = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.GaussianBlur((3,3), sigma=(.2,.6)), transforms.ToTensor()])\n",
        "    self.RSolarize = transforms.Compose([transforms.ToTensor(), transforms.ToPILImage(), transforms.RandomSolarize(230, p=1), transforms.ToTensor()])\n",
        "    self.GNoise = transforms.Compose([transforms.ToTensor(), AddGaussianNoise()])\n",
        "\n",
        "    self.augmentationTransforms = [self.HFlip, self.VFlip, self.RRotation, self.RCrop, self.CJitter, self.RInvert, self.RAdjustSharpness,  \n",
        "                                    self.RAutoContrast, self.REqualize, self.RAffine, self.RPerspective, self.GBlur, self.RSolarize, self.GNoise]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOTG87__YcOY"
      },
      "source": [
        "class balancer():\n",
        "  def __init__(self):\n",
        "    self.aug_transforms = augTransforms()\n",
        "\n",
        "  def balanceClientsDistribution(self, clients, selected_clients):\n",
        "    maxLabelCount = 0\n",
        "    for client in selected_clients:\n",
        "      if len(clients[client].dataset.labels) >= maxLabelCount:\n",
        "        maxLabelCount = len(clients[client].dataset.labels)\n",
        "        labels = clients[client].dataset.labels\n",
        "\n",
        "    values = np.zeros((len(selected_clients), maxLabelCount))\n",
        "\n",
        "    for idx,client in enumerate(selected_clients):\n",
        "      values[idx] = clients[client].dataset.counts\n",
        "\n",
        "    balancedCounts = []\n",
        "    for i in range(len(labels)):\n",
        "      balancedCounts.append(np.max(values[:,i]).astype(int))\n",
        "\n",
        "    for client in selected_clients:\n",
        "      self.balanceDataset(clients[client], labels, balancedCounts)\n",
        "\n",
        "  def balanceDataset(self, client, labels, counts):\n",
        "    balancedDataset = np.empty_like(client.data[0:0])\n",
        "    for label,count in zip(labels,counts):\n",
        "      index = np.where(client.dataset.labels == label)[0][0]  \n",
        "      if(client.dataset.counts[index] >= count):\n",
        "        balancedDataset = np.concatenate((balancedDataset, client.data[client.data[:,1] == label][0:count]), axis=0)\n",
        "      else:\n",
        "        augmentedDataset = self.augment(client.data[client.data[:,1] == label], count - client.dataset.counts[index])\n",
        "        balancedDataset = np.concatenate((balancedDataset, augmentedDataset), axis=0)\n",
        "        balancedDataset = np.concatenate((balancedDataset, client.data[client.data[:,1] == label]), axis = 0)\n",
        "\n",
        "    np.random.shuffle(balancedDataset)\n",
        "    client.assignData(balancedDataset, \"Augmented\")\n",
        "\n",
        "  def augment(self, dataset, augmentationCount):\n",
        "    augmentedDataset = np.empty_like(dataset[0:0])\n",
        "    augmentedData = np.empty_like(dataset[0:1])\n",
        "    count = 0\n",
        "    datasets = [dataset]\n",
        "    index = 0\n",
        "    for dataset in datasets:\n",
        "      for sample in dataset[index:]:  \n",
        "        for transform in self.aug_transforms.augmentationTransforms:\n",
        "          augmentedData[0][0] = np.array(transform(sample[0]).squeeze())\n",
        "          augmentedData[0][1] = sample[1]\n",
        "          augmentedDataset = np.concatenate((augmentedDataset, augmentedData), axis = 0)\n",
        "          count += 1\n",
        "          if count == augmentationCount:\n",
        "            return augmentedDataset \n",
        "\n",
        "      index += 1\n",
        "      datasets.append(augmentedDataset)\n",
        "\n",
        "    np.random.shuffle(augmentedDataset)\n",
        "    return augmentedDataset"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFnU9misjbgD"
      },
      "source": [
        "class Args():\n",
        "  def __init__(self):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    self.num_clients = 20\n",
        "    self.num_clients_round = 5\n",
        "    \n",
        "    self.num_local_epochs = 1\n",
        "    self.batch_size = 8\n",
        "    self.lr = 0.005\n",
        "    \n",
        "    self.data_dir = \"/media/covid_raw/\"\n",
        "    self.labels = ['PNEUMONIA', 'NORMAL', 'COVID', 'LUNG_OPACITY']\n",
        "    self.num_label = 4\n",
        "    self.img_size = 128\n",
        "\n",
        "    self.train_test_split_r = 0.9"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7jXak1GbjTi"
      },
      "source": [
        "args = Args()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jy43DBMROJ_"
      },
      "source": [
        "data = get_training_data(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdVKoCGGJh3-"
      },
      "source": [
        "np.random.shuffle(data)\n",
        "train_set, test_set = np.split(data,[int(args.train_test_split_r*len(data))])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7IjFFNqlrkt"
      },
      "source": [
        "fl_simulator = FLSimulator(train_set, args, \"Non_IID\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ9vFy1nnfMZ"
      },
      "source": [
        "fl_simulator.plotDataDistribution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHAwF7W7V5H_"
      },
      "source": [
        "central_server = server(test_set, fl_simulator.clients, args)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7B01Cdutg4s"
      },
      "source": [
        "central_server.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1KC5FSttnjc"
      },
      "source": [
        "central_server.federated_train(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCBXvZX0wTgM"
      },
      "source": [
        "central_server.test()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}